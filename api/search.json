[{"id":"7de360a3c5a4628d6c3bdcf00d588e7c","title":"一个倒霉蛋的周赛日记","content":"大家好，这里是丹星，今天作为重回周赛的第二周，终于创下了历史记录——拿满18分。但是因为JS在力扣的奇怪Bug，我第四道题被卡在84％，所以面板上还是三道题12分。\n不否认本周的周赛题目确实比较简单，第一名的大佬在9分钟内就完成了四道题，那时候我刚写完第二题。\n\n第一题 最小偶倍数给你一个正整数 n ，返回 2 和 n 的最小公倍数（正整数）。\n\n\n\n\n\n\n\n\n\n输入：n &#x3D; 5\n输出：10\n解释：5 和 2 的最小公倍数是 10 。\n一道侮辱智商的题目，不解释\nvar smallestEvenMultiple = function(n) &#123;\n    if(n % 2 == 0)&#123;\n        return n\n    &#125;else&#123;\n        return n * 2;\n    &#125;\n&#125;;\n\n第二题 最长的字母序连续子字符串的长度字母序连续字符串 是由字母表中连续字母组成的字符串。换句话说，字符串 “abcdefghijklmnopqrstuvwxyz” 的任意子字符串都是 字母序连续字符串 。\n例如，”abc” 是一个字母序连续字符串，而 “acb” 和 “za” 不是。给你一个仅由小写英文字母组成的字符串 s ，返回其 最长 的 字母序连续子字符串 的长度。\n\n\n\n\n\n\n\n\n\n输入：s &#x3D; “abacaba”\n输出：2\n解释：共有 4 个不同的字母序连续子字符串 “a”、”b”、”c” 和 “ab” 。“ab” 是最长的字母序连续子字符串。\n这个也挺简单的，只需要判断第i个字符串和第i-1个字符串是否是连续的就行（根据ASCII码）\nvar longestContinuousSubstring = function(s) &#123;\n    let ans = 1,\n        tmp = 1;\n    for(let i = 1; i &lt; s.length; i++)&#123;\n        if(s[i].charCodeAt() - s[i - 1].charCodeAt() == 1)&#123;\n            tmp++;\n        &#125;else&#123;\n            ans = Math.max(tmp, ans);\n            tmp = 1;\n        &#125;\n    &#125;\n    return Math.max(ans,tmp);\n&#125;;\n第三题 反转二叉树的奇数层给你一棵完美二叉树的根节点 root ，请你反转这棵树中每个 奇数 层的节点值。\n\n例如，假设第 3 层的节点值是 [2,1,3,4,7,11,29,18] ，那么反转后它应该变成 [18,29,11,7,4,3,1,2] 。\n\n反转后，返回树的根节点。\n完美二叉树需满足：二叉树的所有父节点都有两个子节点，且所有叶子节点都在同一层。\n节点的 层数 等于该节点到根节点之间的边数。\n示例 1：\n\n\n\n\n\n\n\n\n\n输入：root &#x3D; [2,3,5,8,13,21,34]\n输出：[2,5,3,8,13,21,34]\n解释：\n这棵树只有一个奇数层。\n在第 1 层的节点分别是 3、5 ，反转后为 5、3 。\n一看到树当然就要想到递归或者迭代啦，或者DFS和BFS。这道题我用的BFS，但是应该还有更好的解法，毕竟我同时用了三个栈。\nvar reverseOddLevels = function(root) &#123;\n    let n = 0,\n        stack = [root],\n        node_stack = [],\n        val_stack = [],\n        flag = 1;\n    while(flag)&#123;\n        if(n % 2 == 0)&#123;\n            // 因为是完美二叉树，所以只要出现节点为null\n            // 就说明到了最后一层，直接跳出\n            if(stack[0].left == null)&#123;\n                flag = 0;\n                break;\n            &#125;\n            while(stack.length)&#123;\n                const node = stack.shift();\n                node_stack.push(node.left, node.right);\n                val_stack.push(node.left.val, node.right.val);\n            &#125;\n            while(val_stack.length)&#123;\n                const node = node_stack.shift(),\n                      val = val_stack.pop();\n                node.val = val;\n                stack.push(node);\n            &#125;\n            n++;\n        &#125;else&#123;\n            // 奇数层，已经完成了逆转，现在stack是满的，\n            // node_stack和val_stack为空，\n            // 我们要把下一个偶数层的node全部存到stack里面，\n            // 也就是替换掉stack里面的节点\n            if(stack[0].left == null)&#123;\n                flag = 0;\n                break;\n            &#125;\n            let currentLen = stack.length;\n            for(let i = 0; i &lt; currentLen; i++)&#123;\n                let tmp = stack.shift();\n                stack.push(tmp.left,tmp.right);\n            &#125;\n            n++;\n        &#125;\n    &#125;\n    return root;\n&#125;;\n第四题 字符串的前缀分数和给你一个长度为 n 的数组 words ，该数组由 非空 字符串组成。\n定义字符串 word 的 分数 等于以 word 作为 前缀 的 words[i] 的数目。\n\n例如，如果 words &#x3D; [“a”, “ab”, “abc”, “cab”] ，那么 “ab” 的分数是 2 ，因为 “ab” 是 “ab” 和 “abc” 的一个前缀。\n\n返回一个长度为 n 的数组 answer ，其中 answer[i] 是 words[i] 的每个非空前缀的分数 总和 。\n注意：字符串视作它自身的一个前缀。\n示例 1：\n\n\n\n\n\n\n\n\n\n输入：words &#x3D; [“abc”,”ab”,”bc”,”b”]\n输出：[5,4,3,2]\n解释：对应每个字符串的答案如下：\n\n“abc” 有 3 个前缀：”a”、”ab” 和 “abc” 。\n2 个字符串的前缀为 “a” ，2 个字符串的前缀为 “ab” ，1 个字符串的前缀为 “abc” 。\n\n总计 answer[0] &#x3D; 2 + 2 + 1 &#x3D; 5 。\n\n“ab” 有 2 个前缀：”a” 和 “ab” 。\n2 个字符串的前缀为 “a” ，2 个字符串的前缀为 “ab” 。\n\n总计 answer[1] &#x3D; 2 + 2 &#x3D; 4 。\n\n“bc” 有 2 个前缀：”b” 和 “bc” 。\n2 个字符串的前缀为 “b” ，1 个字符串的前缀为 “bc” 。\n\n总计 answer[2] &#x3D; 2 + 1 &#x3D; 3 。\n\n“b” 有 1 个前缀：”b”。\n2 个字符串的前缀为 “b” 。\n\n总计 answer[3] &#x3D; 2 。\n这道题最正统的解法应该是字典树，但是——我不会。不过我还是想出了另外一种更加通俗易懂的解法。\n\n首先，遍历每一个单词，用哈希表存储所有的前缀词。如果前缀词存在，则计数+1；如果不存在则计数为1.\n在存储哈希表时，维护一个栈，这个栈里面存储对应单词的所有前缀，所以这个栈存的元素也是列表\n最后，遍历栈，根据哈希表可以查找到对应单词所有前缀和的计数，相加，得到对应位置的值。Javascript版：var sumPrefixScores = function (words) &#123;\n\tif (words.length == 1) &#123;\n\t\treturn [words[0].length];\n\t&#125;\n\tlet map = &#123;&#125;,\n\t\tans = [];\n\tfor (let word of words) &#123;\n\t\tlet right = 1,\n\t\t\ttmp = [];\n\t\twhile (right &lt;= word.length) &#123;\n\t\t\tlet s = word.slice(0, right);\n\t\t\tif (map[s] == undefined) &#123;\n\t\t\t\tmap[s] = 1;\n\t\t\t&#125; else &#123;\n\t\t\t\tmap[s]++;\n\t\t\t&#125;\n\t\t\ttmp.push(s);\n\t\t\tright++;\n\t\t&#125;\n\t\tans.push(tmp);\n\t&#125;\n\tlet res = [];\n\tfor (let arr of ans) &#123;\n\t\tlet sum = 0;\n\t\tarr.forEach(val => &#123;\n\t\t\tsum += map[val];\n\t\t&#125;);\n\t\tres.push(sum);\n\t&#125;\n\treturn res;\n&#125;;\n这个解法在Leetcode第32个用例中爆出莫名错误，但是在本地运行无碍。比赛中我一度认为是这个方法没有考虑到一些特殊情况，但是在周赛结束后，我发现一位前排大佬的Python版本代码和我的思路一致。def sumPrefixScores(words):\n    count =  defaultdict(int)\n    for word in words:\n        m = len(word)\n        for i in range(1, m+1):\n            count[word[:i]] += 1\n            \n    ans = []\n    for word in words:\n        m = len(word)\n        cur = 0\n        for i in range(1, m+1):\n            cur += count[word[:i]]\n        ans.append(cur)\n    return ans\n\n我不管！！！这周周赛我就是满分，这不是超时，我本地代码跑那个例子也只用了2s。\n力扣还我血汗分！！！\n\n\n\n\n\n\n\n\n\n\n欢迎关注这个摸鱼更新的公众号\n\n","slug":"一个倒霉蛋的周赛日记","date":"2022-09-18T10:00:00.000Z","categories_index":"数据结构与算法","tags_index":"数据结构,周赛,二叉树,哈希表","author_index":"风离"},{"id":"ccd980cab5c75bdd94e2c8b8c13252bb","title":"数据结构（四）优先队列/大小根堆","content":"大家好，这里是丹星，又来更新数据结构啦，今天也是老朋友啦，因为之前的几篇文章都提到了它：优先队列，又叫小&#x2F;大根堆。为什么这么快更新呢，因为昨天力扣周赛遇到这个被打爆啦，因为发现了之前总是用sort模拟小根堆也不是办法呀，还是得自己会写。\nWhat is the 堆  ?其实堆不是一个需要我们从头开始定义的一个数据结构，它的本质是用数组去模拟完全二叉树。为什么说是用数组去模拟树呢，因为它并不是真的要去定一个二叉树类，而是本质上还是对数组进行操作。\n二叉树大家都不陌生，那我们讲下以下两个定义。如果二叉树中除了叶子结点，每个结点的度都为 2，则此二叉树称为满二叉树，如：\n\n如果二叉树中除去最后一层节点为满二叉树，且最后一层的结点依次从左到右分布，则此二叉树被称为完全二叉树。\n\n那么一个数组，就可以被看作是一个完全二叉树，也就是一个堆，比如Array &#x3D; array &#x3D; [7,3,8,5,1,2]，变成树就是：\n\n\n\n\n\n\n\n\n\n     7\n     /   \\\n     3     8\n     / \\   /\n     5   1 2\n\nWhat is the 大根堆  ?还是以上面那个数组为例，数组内部最大的值是8，然鹅目前堆顶（Array[0]）是7，所以这并不是一个大根堆。\n大根堆：每个结点的值都大于或等于其左右孩子结点的值\n$So$，上面那个数组要是一个大根堆就必须是:\n\n\n\n\n\n\n\n\n\n     8\n     /   \\\n     5     7\n     / \\   /\n     3   1 2\n     那么，Array从[7,3,8,5,1,2] 变成了 [8,5,7,3,1,2]，你看，我并木有定义一个二叉树，只是交换了数组内部的顺序。\n\n一般来说我们会使得堆顶的元素出堆，这跟队列是一样的，所以这个结构也被叫做优先队列。\n需要补充的几个概念：在完全二叉树中,对于非根节点x都有\n\n父级值在Array数组的下标为： Math.floor(x/2) 或者使用位运算 x &gt;&gt; 2\n左子级值在Array数组的下标为：2x\n右子级值在Array数组的下标为： 2x + 1\n\n还记得我们的堆其实是一个数组么，如果x=0，那么以上三个性质都不会生效，所以我们一般初始化数组的时候使用：arr = [0]，这样所有实际元素的index都会大于1，完美适用以上三条性质。\nShow me your codeclass Heap &#123;\n\tconstructor(compare) &#123;\n\t\tthis.list = [0]; //数组，存放数据\n\t\tthis.compare = typeof compare === 'function' ? compare : this.defaultCompare;\n\t&#125;\n\t// 下方定义左子树，右子树和根节点的获取方式\n\tleft(x) &#123;\n\t\treturn 2 * x;\n\t&#125;\n\tright(x) &#123;\n\t\treturn 2 * x + 1;\n\t&#125;\n\tparent(x) &#123;\n\t\treturn x >> 1;\n\t&#125;\n\n\t//控制堆升序排列还是降序排列\n\tdefaultCompare(a, b) &#123;\n\t\treturn a > b;\n\t&#125;\n\t// 交换x和y对应位置的值\n\tswap(x, y) &#123;\n\t\tconst t = this.list[x];\n\t\tthis.list[x] = this.list[y];\n\t\tthis.list[y] = t;\n\t&#125;\n\t//是否为空\n\tisEmpty() &#123;\n\t\treturn this.list.length === 1;\n\t&#125;\n\t// 数组长度\n\tgetSize() &#123;\n\t\treturn this.list.length - 1;\n\t&#125;\n\t// 最大值\n\ttop() &#123;\n\t\treturn this.list[1];\n\t&#125;\n\t// 删除最大值\n\tpop() &#123;\n\t\tconst &#123; list &#125; = this;\n\t\tif (list.length === 0) return null;\n\t\t// 交换最大值到最后一位，方便pop\n\t\tthis.swap(1, list.length - 1);\n\t\t// 取出最大值\n\t\tconst top = list.pop();\n\t\t// 让第一位重新变成最大值\n\t\tthis.down(1);\n\t\treturn top;\n\t&#125;\n\t// 这个函数使得第一位重新变成最大值\n\tdown(k) &#123;\n\t\tconst &#123; list, left, right, compare &#125; = this;\n\t\tconst size = this.getSize();\n\t\twhile (left(k) &lt;= size) &#123;\n\t\t\t// 下面两行就是把_left取了左子树和右子树偏大的那个\n\t\t\tlet _left = left(k);\n\t\t\tif (right(k) &lt;= size &amp;&amp; compare(list[right(k)], list[_left])) &#123;\n\t\t\t\t_left = right(k);\n\t\t\t&#125;\n\t\t\t// 如果根节点比_left大，就跳出\n\t\t\tif (compare(list[k], list[_left])) return;\n\t\t\t// 否则就交换两个节点，并把k指向_left\n\t\t\tthis.swap(k, _left);\n\t\t\tk = _left;\n\t\t&#125;\n\t&#125;\n\n\t//添加值\n\tpush(val) &#123;\n\t\t// 新增数据，向堆尾添加\n\t\tthis.list.push(val);\n\n\t\tthis.up(this.list.length - 1);\n\t&#125;\n\tup(k) &#123;\n\t\tconst &#123; list, parent, compare &#125; = this;\n\t\t// 第k个元素和它的根节点比较\n\t\twhile (k > 1 &amp;&amp; compare(list[k], list[parent(k)])) &#123;\n\t\t\t// 交换x和y对应位置的值\n\t\t\tthis.swap(parent(k), k);\n\t\t\tk = parent(k);\n\t\t&#125;\n\t&#125;\n&#125;\n万恶的力扣真题OK，你已经学会了如何构建一个大根堆，让我们来看一下这万恶的周赛第3题，建堆代码如上，所以我省略。\n\nvar minGroups = function(intervals) &#123;\n    // 小根堆\n    let tmp = new Heap((a,b)=>a&lt;b);\n    intervals.sort((a,b)=> a[0]-b[0] || a[1] - b[1]);\n    for(let arr of intervals)&#123;\n        const [x,y] = arr;\n        if(tmp.list.length == 1 || tmp.list[1] >= x)&#123;\n            tmp.push(y);\n        &#125;else&#123;\n            tmp.pop();\n            tmp.push(y);\n        &#125;\n    &#125;\n    return tmp.getSize();\n&#125;;\n是不是很简单呀，短短十几行代码（不算建堆的80来行代码），所以JS为什么没有内置的数据结构库，连C++都是调的heapq函数！！！！\n所以以下来一个我的老本行Python版本\nfrom heapq import *\ndef minGroups(self, intervals: List[List[int]]) -> int:\n    tmp = []\n    intervals.sort()\n    for x, y in intervals:\n        if len(tmp) == 0 or tmp[0] >= x:\n            heappush(tmp, y)\n        else:\n            heappop(tmp)\n            heappush(tmp, y)\n    return len(tmp)\n万恶的Python仗着自己库多就沾沾自喜，殊不知我们Javascript多写了80多行代码，时间快了40ms，达到了质的飞跃,但是为什么多用了30MB的空间？\n\n\n\n\n\n\n\n\n\n\n\n欢迎关注这个摸鱼更新的公众号\n\n","slug":"优先队列","date":"2022-09-12T10:00:00.000Z","categories_index":"数据结构与算法","tags_index":"数据结构,优先队列","author_index":"风离"},{"id":"8ff685f8218119a3e3b228fe1d1d8ae5","title":"字节跳动2022年9月模拟笔试（上）","content":"9月排名赛代码总结第一题：炮弹传感器2为了研究炮弹的射击轨迹，研究人员在炮弹上安装了一个高精度的高度传感器，每隔一小段时间记录一下当前的海拔高度。研究人员需要找到一个快速的算法，可以方便的找到曾经飞到过某一个高度的炮弹数据。\n输入描述:第一行为正整数m，表示有m组测试用例，其中1&lt;&#x3D;m &lt;&#x3D;10000接下来2m行，每2行代表一组测试用例\n其中，前一行为正整数n, k，分别代表数据传感器的数据条数和目标高度，其中1&lt;&#x3D;n &lt;&#x3D; 10000，1 &lt;&#x3D;k &lt;&#x3D;10000o\n后一行有n个正整数，编号为[0,… , n-1]，代表数据传感器的高度数据，先升后降，而且只有可能在最高点存在相邻数据高度一样的情况，其余地方不存在相邻数据高度一致的情况，其中每个数据均满足[0,100000]\n输出描述:每行输出一个炮弹是否飞到过当前高度，如果是，打标输出在采样数据的第几个到第几个数据是在该高度及以上\n示例输入5\n9 4\n1 2 3 4 5 5 4 3 2\n9 14\n1 2 3 4 5 5 4 3 2\n11 7\n1 2 3 4 5 6 7 5 4 3 2\n1 14\n1\n1 1\n1\n输出true 3 6\nfalse\ntrue 6 6\nFalse\ntrue 0 0\n\n解法\n\n思路：双指针，两端分别在0和n-1分别开始，直到高度到达k。\n\n正常情况是i和j分别在两边，即i &lt;= j，输出true i j；\n\n若i &gt; j，则没有到达指定高度，输出false。\n\n\n\n\nconst readline &#x3D; require(&#39;readline&#39;);\nconst rl &#x3D; readline.createInterface(&#123;\n        input: process.stdin,\n        output: process.stdout,\n&#125;);\nlet cur_line &#x3D; 0;\nlet k;\nrl.on(&#39;line&#39;, line &#x3D;&gt; &#123;\n        if (cur_line &#x3D;&#x3D; 0) &#123;\n                cur_line +&#x3D; 1;\n                return;\n        &#125;\n        if (cur_line % 2 !&#x3D;&#x3D; 0) &#123;\n                &#x2F;&#x2F; 奇数行得到k\n                k &#x3D; parseInt(line.split(&#39; &#39;)[1]);\n        &#125; else &#123;\n                let arr &#x3D; line.split(&#39; &#39;).map(val &#x3D;&gt; parseInt(val));\n                console.log(&#39;******答案输出：&#39;, findheight(arr, k));\n        &#125;\n        cur_line +&#x3D; 1;\n&#125;);\n\nfunction findheight(arr, k) &#123;\n        let i &#x3D; 0,\n                n &#x3D; arr.length,\n                j &#x3D; n - 1;\n        while (i &lt; n &amp;&amp; arr[i] &lt; k) i++;\n        while (i &gt; 0 &amp;&amp; arr[j] &lt; k) j--;\n        if (i &gt; j) &#123;\n                return false;\n        &#125; else &#123;\n                return &#96;true $&#123;i&#125; $&#123;j&#125;&#96;;\n        &#125;\n&#125;\n\n第二题：取餐次序计算防疫期间，为了降低传播风险，公司食堂启用线上订餐、通知取餐的策略，公司员工需要在线上提交就餐需求，并填写对取餐时间安排的容忍度（1-9个等级)，容忍度越低表示越想拿到饭，会被更早通知取餐，就餐需求提交后会按照提交顺序生成订单(订单号从1开始递增)，通知取餐的工作人员的工作模式如下:拿到按顺序生成的订单列表后开始翻阅，如果某订单容忍度在剩余订单中是最低的，就通知该订单的下单员工取餐，否则将订单移至列表尾部，已知某日午餐的订单列表和某个订单号，计算该订单会被安排在第几位取餐。\n输入描述:第一行两个整数，分别是订单总数m (m&lt;&#x3D;1000)、订单号n(从1开始)\n第二行是订单列表对应的容忍度列表\n输出描述:输出一个整数表示订单号n的实际取餐次序\n示例14 3\n1 2 3 4\n输出3\n说明总共4个订单，自己的订单号是3（从1开始），按订单号排序的4个订单的容忍度分别是1、2、3、4，那么自己会被安排在第3位取餐。\n示例27 3\n8 7 8 9 1 2 8\n输出4\n说明总共7个订单，自己的订单号是3(从1开始)，按订单号排序的4个订单的容忍度分别是8、7、8、9、1、2、8，那么自己会被安排在第4位取餐\n第一位取餐的容忍度是1\n第二位取餐的容忍度是2\n第三位取餐的容忍度是7\n第四位取餐的容忍度是8，即自己\n解法：思路：优先队列，但是JS没有内置的优先队列库，我不想自己写，而且我实在看不懂C++版的答案，所以下面用了sort函数直接排了个序，当作优先队列，然后维护一个指针myTurnIndex，保证可以知道自己的订单号到底在哪个位置，测了几组数据，应该是和答案一致的。\nconst readline &#x3D; require(&#39;readline&#39;);\nconst rl &#x3D; readline.createInterface(&#123;\n    input: process.stdin,\n    output: process.stdout,\n&#125;);\nlet cur_line &#x3D; 0;\nlet k;\nrl.on(&#39;line&#39;, line &#x3D;&gt; &#123;\n    if (cur_line &#x3D;&#x3D;&#x3D; 0) &#123;\n        k &#x3D; parseInt(line.split(&#39; &#39;)[1]) - 1;\n        cur_line +&#x3D; 1;\n    &#125; else if (cur_line &#x3D;&#x3D;&#x3D; 1) &#123;\n        let arr &#x3D; line.split(&#39; &#39;).map(val &#x3D;&gt; parseInt(val));\n        &#x2F;&#x2F; console.log(arr, k);\n        console.log(&#39;******答案输出：&#39;, main(arr, k));\n    &#125;\n&#125;);\n\nfunction main(arr, k) &#123;\n    let myTurnIndex &#x3D; k,\n        myTurn &#x3D; arr[k],\n        count &#x3D; 1;\n    heapq &#x3D; [];\n    arr.forEach(element &#x3D;&gt; &#123;\n        heapq.push(element);\n    &#125;);\n    heapq.sort((a, b) &#x3D;&gt; a - b);\n    console.log(heapq);\n    console.log(arr);\n    while (true) &#123;\n        let tmp &#x3D; arr.shift();\n        &#x2F;&#x2F; console.log(&#96;arr:$&#123;arr&#125;，tmp:$&#123;tmp&#125;，myTurnIndex：$&#123;myTurnIndex&#125;&#96;);\n        if (tmp &#x3D;&#x3D;&#x3D; heapq[0] &amp;&amp; tmp &#x3D;&#x3D;&#x3D; myTurn &amp;&amp; myTurnIndex &#x3D;&#x3D;&#x3D; 0) &#123;\n            return count;\n        &#125;\n        if (tmp &#x3D;&#x3D;&#x3D; heapq[0]) &#123;\n            &#x2F;&#x2F; console.log(&#39;-----&#39;);\n            count +&#x3D; 1;\n            heapq.shift();\n            myTurnIndex--;\n            continue;\n        &#125;\n        &#x2F;&#x2F; 这里开始就是不符合最小容忍度的了\n        &#x2F;&#x2F; 如果是自己\n        if (tmp &#x3D;&#x3D;&#x3D; myTurn &amp;&amp; myTurnIndex &#x3D;&#x3D;&#x3D; 0) &#123;\n            arr.push(tmp);\n            myTurnIndex &#x3D; heapq.length - 1;\n        &#125; else &#123;\n            &#x2F;&#x2F; 如果不是自己\n            arr.push(tmp);\n            myTurnIndex--;\n        &#125;\n    &#125;\n&#125;","slug":"字节跳动九月模拟笔试（上）","date":"2022-09-07T13:57:00.000Z","categories_index":"数据结构与算法","tags_index":"数据结构,优先队列,字节跳动,双指针","author_index":"风离"},{"id":"66b66ee279145d8f8b0e567a448e4840","title":"爬虫前置——页面和网络相关知识","content":"HTML什么是HTML？\n\n超文本标记语言（英语：HyperText Markup Language，简称：HTML）是一种用于创建网页的标准标记语言。\n\n前端三件套：HTML、CSS和Javascript。\nHTML和CSS不算是编程语言。HTML更像是一个Word，可以写文字、插入图片、表格、有序与无序列表。只不过只有HTML会很丑，我们可以忍受读论文的时候使用Word，但我们不希望逛淘宝如同看一个Word。\n所以CSS使得HTML做出来的“Word”排版更加丰富。\n淘宝的页面\n淘宝去掉CSS的页面\nHTML、CSS和Javascript的关系\n HTML代码在这~~ CSS代码在这~~\nR中解析HTML文件在R语言中，我们通常使用XML包中的htmlParse函数来解析一个HTML文件。\nlibrary(XML)\nurl &lt;- &quot;http:&#x2F;&#x2F;www.r-datacollection.com&#x2F;materials&#x2F;html&#x2F;JavaScript.html&quot;\nexample1 &lt;- htmlParse(file &#x3D; url)\nprint(example1)\n\n解析后的HTML在R中打印出来\n\n","slug":"爬虫前置——网页和网络相关","date":"2022-10-10T07:00:00.000Z","categories_index":"前端","tags_index":"HTML,CSS,HTTP,XML","author_index":"风离"},{"id":"8ff685f8218119a3e3b228fe1d1d8ae5","title":"字节跳动2022年9月模拟笔试（上）","content":"第三题：数组游戏\n\n\n\n\n\n\n\n\n双休在家的凯凯真的是太无聊了，他准备和他家的猫玩一个游戏。\n凯凯在小黑板上写下一串有正有负的数列，猫咪从左到右，每碰到一个数，可以选择选取或者不选取。\n在选取过程中，要保证所有选取的数的和始终为非负。在这个限制条件下求最多可以选取多少个数。\n小猫咪表示“我太难了”\n你能帮帮它么?\n\n\n\n\n\n\n\n\n\n输入描述：会有多组询问\n首先输入一个数字接下来有组数据\n每组数据里，首先会有一个数，表示接下来这个数列的长度为\n然后接下来一行会有n个数字，从左到右表示题目所说的数列，每个数字\n输出描述：对于每一个提问，请依次输出最多可以选取多少个数\n备注：每个数字（）（\n\n\n\n\n\n\n\n\n\n示例输入2\n6\n4 -4 1 -3 -1 -3\n5\n1 2 3 4 5\n输出5\n5\n说明第一组数据：选取1，3，4，5，6个数；\n第二组数据，全部正数，那就全选。\n解法\n\n\n\n\n\n\n\n\n注意：必须从左到右选取，不能直接排序选！！！！！必须保证中间过程所选数之和非负！！！\n所以进行模拟，使用反悔贪心+优先队列（没错，又是他），规则如下：\n\n遇到正数或者零：直接选\n遇到负数\n选了之后sum &gt;&#x3D; 0，那么直接选，但是要存入优先队列（小根堆）中\n选了之后sum &lt; 0，那么在已经选择的负数中置换出一个最小的（前提是当前这个数是比已选负数的最小值要大的）\n\n注：因为JS没有内置的优先队列函数，我不想自己写一个，所以这里每次在队列加入元素都会使用一次sort函数，这样可以保证维护一个小根堆。\nconst rl &#x3D; readline.createInterface(&#123;\n    input: process.stdin,\n    output: process.stdout,\n&#125;);\nlet cur_line &#x3D; 0;\nrl.on(&#39;line&#39;, line &#x3D;&gt; &#123;\n    if (cur_line &lt; 2) &#123;\n        cur_line +&#x3D; 1;\n        return;\n    &#125;\n    if (cur_line % 2 &#x3D;&#x3D;&#x3D; 0) &#123;\n        let arr &#x3D; line.split(&#39; &#39;).map(val &#x3D;&gt; parseInt(val));\n        &#x2F;&#x2F; console.log(arr, k);\n        console.log(&#39;******答案输出：&#39;, main(arr));\n    &#125;\n    cur_line++;\n&#125;);\n\nfunction main(arr) &#123;\n    let sum &#x3D; 0,\n        count &#x3D; 0,\n        q &#x3D; [];\n    while (arr.length) &#123;\n        let tmp &#x3D; arr.shift();\n        &#x2F;&#x2F; console.log(&#96;tmp:$&#123;tmp&#125;,q:$&#123;q&#125;,sum:$&#123;sum&#125;&#96;);\n        if (tmp &gt;&#x3D; 0) &#123;\n            sum +&#x3D; tmp;\n            count++;\n        &#125; else &#123;\n            if (sum + tmp &gt;&#x3D; 0) &#123;\n                sum +&#x3D; tmp;\n                q.push(tmp);\n                q.sort((a, b) &#x3D;&gt; a - b);\n                count++;\n            &#125; else &#123;\n                if (q.length !&#x3D; 0 &amp;&amp; q[0] &lt; tmp) &#123;\n                    sum &#x3D; sum - q.shift() + tmp;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n    return count;\n&#125;\n\n第四题：寻找对称的二叉树特定节点\n\n\n\n\n\n\n\n\n\n给定一颗二叉树，二叉树每个节点都有一个唯一的整数值代表节点，在遍历时，我们使用节点的整数值作为标记;结构对称，是指二叉树从根节点往下看，左右翻转一下，能够重合(不考虑节点内容比较，仅仅是结构)，我们就称这棵二叉树树结构对称\n输入:二叉树的节点个数N (0&lt;N&lt;60000)、前序和中序遍历结果，分别是第一行、第二行与第三行;各个节点整数值在1到60000之间\n输出︰判断这棵二叉树是否结构对称，若对称请输出最大值节点在树中对称节点的整数值，不对称请直接输出最大值节点的整数值\n\n\n\n\n\n\n\n\n\n\n输入描述二叉树的前序和中序遍历结果，以数组序列表示\n第一行为节点个数N (0&lt;N&lt;60000)前序和中序遍历结果，输入分别是第二行与第三行\n输出描述判断这棵二叉树是否结构对称，若对称请输出最大值节点在树中对称节点的整数值，不对称请直接输出最大值节点的整数值\n备注1.每个节点都有一个唯一的整数值代表节点（但比赛中实际数据似乎并不是的），结构对称不考虑节点内容数字大小比较，仅仅是结构\n2.根节点相对自己是对称的\n\n\n\n\n\n\n\n\n\n示例1输入3\n1 3 4\n3 1 4\n输出3\n说明这颗二叉树根是1，左右子节点分别是3和4，是结构对称的，4是最大值节点，其对称节点是3，所以最后输出为3。\n示例2输入5\n1 3 5 7 2\n5 3 1 2 7\n输出7\n说明这颗二叉树，节点7是数值最大的节点，但其节点2对称位置没有节点，二叉树不对称，所以输出为7。\n解法\n首先，根据前序遍历和中序遍历重建二叉树，这个具体怎么做看上篇公众号文章。\n然后，递归判断是否结构对称\n最后，如果对称，则输出最大值的对称节点；若不对称，则输出最大的节点值。\n\nconst &#123; func &#125; &#x3D; require(&#39;prop-types&#39;);\nconst readline &#x3D; require(&#39;readline&#39;);\nconst rl &#x3D; readline.createInterface(&#123;\n    input: process.stdin,\n    output: process.stdout,\n&#125;);\n\nlet cur_line &#x3D; 0,\n    preorder,\n    inorder;\nrl.on(&#39;line&#39;, line &#x3D;&gt; &#123;\n    if (cur_line &#x3D;&#x3D; 0) &#123;\n        cur_line++;\n        return;\n    &#125; else if (cur_line &#x3D;&#x3D; 1) &#123;\n        preorder &#x3D; line.split(&#39; &#39;).map(val &#x3D;&gt; parseInt(val));\n        cur_line++;\n    &#125; else if (cur_line &#x3D;&#x3D; 2) &#123;\n        inorder &#x3D; line.split(&#39; &#39;).map(val &#x3D;&gt; parseInt(val));\n        console.log(main(preorder, inorder));\n    &#125;\n&#125;);\n\n&#x2F;&#x2F; 定义二叉树\nfunction TreeNode(val, left, right) &#123;\n    this.val &#x3D; val &#x3D;&#x3D;&#x3D; undefined ? 0 : val;\n    this.left &#x3D; left &#x3D;&#x3D;&#x3D; undefined ? null : left;\n    this.right &#x3D; right &#x3D;&#x3D;&#x3D; undefined ? null : right;\n&#125;\n\n&#x2F;&#x2F; 递归重建二叉树\nfunction buildTree(preorder, inorder) &#123;\n    if (preorder.length &#x3D;&#x3D;&#x3D; 0) return null;\n    let mid_index &#x3D; inorder.indexOf(preorder[0]);\n    let root &#x3D; new TreeNode(preorder[0]);\n    root.left &#x3D; buildTree(preorder.slice(1, mid_index + 1), inorder.slice(0, mid_index));\n    root.right &#x3D; buildTree(preorder.slice(mid_index + 1), inorder.slice(mid_index + 1));\n    return root;\n&#125;\n\n&#x2F;&#x2F; 递归检查树的结构是否对称（无需比较值）\nfunction check(left, right) &#123;\n    &#x2F;&#x2F; 如果都为null，那么对称，返回1\n    if (!left &amp;&amp; !right) return 1;\n    &#x2F;&#x2F; 如果只有一方为null，那不对称，返回0\n    if (!left || !right) return 0;\n    return check(left.left, right.right) &amp;&amp; check(left.right, right.left);\n&#125;\n\nfunction main(preorder, inorder) &#123;\n    &#x2F;* 这个查找对称二叉树的对称节点写在了主函数里面，\n        因为如果写在外面，那么必须传入max_value，然后在函数内部定义pair_val并返回\n        写在主函数内部的一个好处是，只要保证max_value和pair_val和findPair在同一作用域\n        那么findPair内部就会向外层自动寻找这两个值，类似于闭包。\n     *&#x2F;\n    function findPair(left, right) &#123;\n        if (!left || !right) return;\n        if (left.val &#x3D;&#x3D;&#x3D; max_value) &#123;\n            pair_val &#x3D; right.val;\n        &#125;\n        if (right.val &#x3D;&#x3D;&#x3D; max_value) &#123;\n            pair_val &#x3D; left.val;\n            return pair_val;\n        &#125;\n        &#x2F;&#x2F; 继续往下找\n        findPair(left.left, right.right);\n        findPair(left.right, right.left);\n    &#125;\n\n    &#x2F;&#x2F; 最大节点值对称点的值，先声明\n    &#x2F;&#x2F; 这里注意，必须在和findPair函数一级的作用域定义，\n    &#x2F;&#x2F; 因为如果在let在下方的if语句中定义，findPair会找不到pair_val\n    let pair_val;\n    &#x2F;&#x2F; 最大节点值\n    let max_value &#x3D; Math.max(...preorder);\n    let root &#x3D; buildTree(preorder, inorder);\n    let flag &#x3D; check(root.left, root.right);\n    if (flag) &#123;\n        &#x2F;&#x2F; 对称\n        findPair(root.left, root.right); &#x2F;&#x2F; 注意findPair的主要目的是修改pair_val，而非返回一个结果\n        return pair_val;\n    &#125; else &#123;\n        &#x2F;&#x2F; 非对称\n        return max_value;\n    &#125;\n&#125;\n\n\n\n\n\n\n\n\n\n\n本次9月排名赛的四道题都放在了飞书文档公开，欢迎访问：\nhttps://bytedancecampus1.feishu.cn/docx/doxcnz5i6ZoB0PYNkUCEmoEXyRg\n\n","slug":"字节跳动九月模拟笔试（下）","date":"2022-09-08T13:57:00.000Z","categories_index":"数据结构与算法","tags_index":"数据结构,二叉树,优先队列,字节跳动","author_index":"风离"},{"id":"e0afe5470d30cea57d222ddde269ba0c","title":"JSONP解决跨域问题","content":"JSONP是JSON with Padding的略称，JSONP为民间提出的一种跨域解决方案，通过客户端的script标签发出的请求方式。\n什么时候才有跨域问题?浏览器的ajax，去请求不同的源的数据，就会出现跨域问题。\n问: img&#x2F;srcipt标签的src有跨域问题吗?\n答:没有\n所以可以利用这个解决跨域\n\n&lt;!--index.html --&gt;\n&lt;body&gt;\n    &lt;button id&#x3D;&quot;btn&quot;&gt;发送ajax请求&lt;&#x2F;button&gt;\n    &lt;script&gt;\n        &#x2F;&#x2F; 这个例子不使用Ajax请求，而是使用script标签，向服务器发送请求\n        &#x2F;&#x2F; 然后在客户端得到对应的数据\n\n        let btn &#x3D; document.querySelector(&#39;#btn&#39;);\n\n        function jsonp(options) &#123;\n            &#x2F;&#x2F; 定义一个回调函数\n            let callBackName &#x3D; &#39;wangcai&#39;;\n            &#x2F;&#x2F; 回调函数负责返回数据或者错误\n            window[callBackName] &#x3D; function (data) &#123;\n                if (data !&#x3D; null) &#123;\n                    options.success(data);\n                &#125; else &#123;\n                    options.fail();\n                &#125;\n            &#125;\n\n            let url &#x3D; options[&#39;url&#39;] + &#39;?callBack&#x3D;&#39; + callBackName;\n\n            let scriptEle &#x3D; document.createElement(&#39;script&#39;);\n            scriptEle.src &#x3D; url;\n            document.body.append(scriptEle);\n        &#125;\n\n        btn.onclick &#x3D; function () &#123;\n            jsonp(&#123;\n                url: &#39;http:&#x2F;&#x2F;localhost:3000&#x2F;&#39;,\n                success: function (data) &#123;\n                    console.log(&quot;data&quot;, data);\n                &#125;,\n                fail: function (err) &#123;\n                    console.log(&quot;数据请求失败&quot;);\n                &#125;\n            &#125;)\n        &#125;\n    &lt;&#x2F;script&gt;\n&lt;&#x2F;body&gt;\n\n服务端的代码\n&#x2F;&#x2F; service.js\n\nlet express &#x3D; require(&#39;express&#39;);\n\nlet app &#x3D; express();\n\n&#x2F;&#x2F; 解决跨域问题\n&#x2F;&#x2F; app.all(&#39;*&#39;, function (req, res, next) &#123;\n&#x2F;&#x2F;     &#x2F;&#x2F; 设置允许跨域的域名,*代表允许任意域名跨域\n&#x2F;&#x2F;     res.header(&#39;Access-Control-Allow-Origin&#39;, &#39;*&#39;);\n&#x2F;&#x2F;     &#x2F;&#x2F; 允许的header类型\n&#x2F;&#x2F;     res.header(&#39;Access-Control-Allow-Headers&#39;, &#39;content-type&#39;);\n&#x2F;&#x2F;     &#x2F;&#x2F; 跨域允许的请求方式\n&#x2F;&#x2F;     res.header(&#39;Access-Control-Allow-Methods&#39;, &#39;DELETE,PUT,POST,GET,OPTIONS&#39;);\n&#x2F;&#x2F;     if (req.method.toLowerCase() &#x3D;&#x3D; &#39;options&#39;) res.send(200); &#x2F;&#x2F; 让options 尝试请求快速结束\n&#x2F;&#x2F;     else next();\n&#x2F;&#x2F; &#125;);\n\n&#x2F;&#x2F; 改了服务器代码一定要重启服务器\n&#x2F;&#x2F; req是请求，res是响应\napp.get(&#39;&#x2F;&#39;, (req, res) &#x3D;&gt; &#123;\n    &#x2F;&#x2F; 当访问&#x2F; 时，响应json数据\n    &#x2F;&#x2F; res.json(&#123;\n    &#x2F;&#x2F;  name: &#39;wc&#39;,\n    &#x2F;&#x2F;  age: 18,\n    &#x2F;&#x2F; &#125;);\n\n    &#x2F;&#x2F; 响应字符串\n    &#x2F;&#x2F; res.send(&#39;hello express&#39;);\n\n    &#x2F;&#x2F; 响应一个函数调用的字符串\n    res.send(&#96;$&#123;req.query.callBack&#125;(\n        $&#123;JSON.stringify(&#123; name: &#39;z3&#39;, age: 128 &#125;)&#125;\n    )&#96;);\n&#125;);\n\n&#x2F;&#x2F; IP 127.0.0.1\n&#x2F;&#x2F; 域名  localhost\n&#x2F;&#x2F; 端口 3000\napp.listen(3000, () &#x3D;&gt; &#123;\n    console.log(&#39;server is running on 3000&#39;);\n&#125;);\n\nJSONP和AJAX请求的异同相同点：\n使用的目的一致，都是客户端向服务端请求数据，将数据拿回客户端进行处理。\n\n不同点：\najax请求是一种官方推出的请求方式，通过xhr对象去实现，jsonp是民间发明，script标签实现的请求。\najax是一个异步请求，jsonp是一个同步请求\najax存在同源检查，jsonp不存在同源检查，后端无需做解决跨域的响应头。\najax支持各种请求的方式，而jsonp只支持get请求\najax的使用更加简便，而jsonp的使用较为麻烦。\n\n","slug":"JSONP解决跨域","date":"2022-08-20T12:17:00.000Z","categories_index":"前端","tags_index":"网络请求,JSONP,Javascript","author_index":"风离"},{"id":"ecfdc9266c75f1a5dbb47c4430fd00a9","title":"数据竞赛实战（一）点击反欺诈预测，2022年3月第二名","content":"大家好，这里是丹星，今天的项目是我之前比赛的代码啦，这个分数还是比较靠前的，总排名应该能有前20吧，代码我4月份其实已经开源在AI Studio了（https://aistudio.baidu.com/aistudio/projectdetail/3735300）。\n比赛介绍广告欺诈是数字营销需要面临的重要挑战之一，点击会欺诈浪费广告主大量金钱，同时对点击数据会产生误导作用。本次比赛提供了约50万次点击数据。特别注意：我们对数据进行了模拟生成，对某些特征含义进行了隐藏，并进行了脱敏处理。请预测用户的点击行为是否为正常点击，还是作弊行为。点击欺诈预测适用于各种信息流广告投放，banner广告投放，以及百度网盟平台，帮助商家鉴别点击欺诈，锁定精准真实用户。本项目使用LightGBM和XGBoost模型进行最后分类，分数分别为89.29和89.31，结果取平均后分数为最后分数89.3213。之前的特征工程也借鉴了AI Studio其他的一些开源方法，在此表示感谢。以下为变量说明。\n\n\n\n字段\n类型\n说明\n\n\n\nsid\nstring\n样本id&#x2F;请求会话sid\n\n\npackage\nstring\n媒体信息，包名（已加密）\n\n\nversion\nstring\n媒体信息，app版本\n\n\nandroid_id\nstring\n媒体信息，对外广告位ID（已加密）\n\n\nmedia_id\nstring\n媒体信息，对外媒体ID（已加密）\n\n\napptype\nint\n媒体信息，app所属分类\n\n\ntimestamp\nbigint\n请求到达服务时间，单位ms\n\n\nlocation\nint\n用户地理位置编码（精确到城市）\n\n\nfea_hash\nint\n用户特征编码（具体物理含义略去）\n\n\nfea1_hash\nint\n用户特征编码（具体物理含义略去）\n\n\ncus_type\nint\n用户特征编码（具体物理含义略去）\n\n\nntt\nint\n网络类型 0-未知, 1-有线网, 2-WIFI, 3-蜂窝网络未知, 4-2G, 5-3G, 6–4G\n\n\ncarrier\nstring\n设备使用的运营商 0-未知, 46000-移动, 46001-联通, 46003-电信\n\n\nos\nstring\n操作系统，默认为android\n\n\nosv\nstring\n操作系统版本\n\n\nlan\nstring\n设备采用的语言，默认为中文\n\n\ndev_height\nint\n设备高\n\n\ndev_width\nint\n设备宽\n\n\ndev_ppi\nint\n屏幕分辨率\n\n\n比赛代码数据加载import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nimport pickle\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 50)  #最多显示5列\npd.set_option('display.max_rows', 150)#最多显示10行\n# 数据加载\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test1.csv')\ntest = test.iloc[:, 1:]\ntrain = train.iloc[:, 1:]\n\nall_df = pd.concat([train,test])\n\n特征工程# 长度特征\nall_df['fea1_hash_len'] = all_df['fea1_hash'].map(lambda x:len(str(x)))\nall_df['fea_hash_len'] = all_df['fea_hash'].map(lambda x:len(str(x)))\n\n# 设备高宽比、设备面积\nall_df['dev_rate'] = all_df['dev_height']/all_df['dev_width']\nall_df['dev_area'] = all_df['dev_height']*all_df['dev_width']\n\n# 面积与分辨率比\nall_df['dev_area_ppi'] = all_df['dev_ppi']/all_df['dev_area']\nall_df['dev_area_ppi'][np.isinf(all_df['dev_area_ppi'])] = 0\n\n# 利用数量特征构造。\ndef unique(x):\n    result = pd.value_counts(x)\n    x= [result[each] for each in x]\n    return x\n\n\nfor f in ['dev_height','dev_width','media_id','package','apptype',\n          'android_id','fea1_hash','fea_hash']:\n    all_df[f+'_value_count']=unique(all_df[f])\n\n\n# lan的处理\n# set(test['lan'].unique()) - set(train['lan'].unique()) &#123;'in_id', 'zh-us'&#125;\n# 测试集里面多出来了这两个&#123;'in_id', 'zh-us'&#125;，结果查询，这个语言号不存在，所以我们等会筛选出预测集中这两个的样本，直接判断为欺诈(样本只有两个)\nle = LabelEncoder()\nall_df['lan']= all_df['lan'].astype('str')\nall_df['lan'] = le.fit_transform(all_df['lan'])\n\ntrain = all_df[all_df['label'].notnull()]\ntest = all_df[all_df['label'].isnull()]\n\n\n# 处理osv\ndef trans_osv(osv):\n    osv = str(osv).replace(' ','').replace('.','').replace('Android_','').replace('十核20G_HD','').replace('Android','').replace('W','')\n#     if osv == 'nan' or osv == 'GIONEE_YNGA':\n#         result = 810\n    if '_' in osv:\n        osv = osv.split('_')[0]\n    return osv\n\ntrain['osv'] = train['osv'].apply(trans_osv)\nidx = train['osv'].value_counts()[train['osv'].value_counts() &lt; 1000].index.tolist()\ntrain['osv'] = train['osv'].apply(lambda x:'other' if x in idx else x)\ntrain['osv'] = train['osv'].astype('object')\n\ntest['osv'] = test['osv'].apply(trans_osv)\ntest['osv'] = test['osv'].apply(lambda x:'other' if x in idx else x)\ntest['osv'] = test['osv'].astype('object')\n\n# set(test['lan'].unique()) - set(train['lan'].unique())   &#123;'120', '446', '71300', 'GIONEE'&#125;\n# 这四个东西训练集没有出现，直接判定为欺诈(4个样本)\n\nle = LabelEncoder()\nle.fit(list(set(test['osv'].unique()) | set(train['osv'].unique())))\ntrain['osv'] = le.transform(train['osv'])\ntest['osv'] = le.transform(test['osv'])\n\n# 直接判定为欺诈的sid\nTrick_id = [1260961, 1393628, 1356887, 1911347,1298847, 1629795]\n\ntrain.fillna(-1,inplace=True)\ntest.fillna(-1,inplace=True)\n\n# os不要，都是安卓\ntrain.drop('os',axis=1,inplace=True)\ntest.drop('os',axis=1,inplace=True)\n\n# 时间类特征\n\nfrom datetime import datetime\ntrain['timestamp'] = train['timestamp'].apply(lambda x : datetime.fromtimestamp(x/1000))\ntest['timestamp'] = test['timestamp'].apply(lambda x : datetime.fromtimestamp(x/1000))\n# train和test都是从2019-06-03凌晨到2019-06-10凌晨\n\n# 分解时间\ntrain['day'] = train['timestamp'].dt.day\ntrain['weekday'] = train['timestamp'].dt.weekday\ntrain['hour'] = train['timestamp'].dt.hour\ntrain['minute'] = train['timestamp'].dt.minute\n\ntest['day'] = test['timestamp'].dt.day\ntest['weekday'] = test['timestamp'].dt.weekday\ntest['hour'] = test['timestamp'].dt.hour\ntest['minute'] = test['timestamp'].dt.minute\n\nstart_time = train['timestamp'].min()\ntrain['timestamp_diff'] = train['timestamp']-start_time\ntrain['timestamp_diff'] = train['timestamp_diff'].dt.days*24 + train['timestamp_diff'].dt.seconds/3600 # 按小时来计算\ntest['timestamp_diff'] = test['timestamp']-start_time\ntest['timestamp_diff'] = test['timestamp_diff'].dt.days*24 + test['timestamp_diff'].dt.seconds/3600 # 按小时来计算\n\n\n# 对时间差做分桶\ntest['timestamp_diff'] = test['timestamp_diff'].apply(lambda x: int(x//13))\ntrain['timestamp_diff'] = train['timestamp_diff'].apply(lambda x: int(x//13))\n\ntrain.drop('timestamp', axis=1,inplace=True)\ntest.drop('timestamp', axis=1,inplace=True)\n\n#特征清洗version\ndef rep(x):\n    if str(x).isdigit():\n        return int(x)\n    elif str(x)[0] == \"v\" or \"V\":\n        if str(x)[1:].isdigit():\n            return int(str(x)[1:])\n        else:\n            return 0\n    else:\n        return 0\n\ntrain['version'] = train['version'].apply(rep)\ntest['version'] = test['version'].apply(rep)\n\n#类别特征\ncate_features = ['apptype','carrier','ntt','location','cus_type','media_id',\n'dev_width','dev_height','android_id','fea1_hash']\n\n# 构造fea_hash_len特征\ntrain['fea_hash_len'] = train['fea_hash'].map(lambda x: len(str(x)))\ntrain['fea1_hash_len'] = train['fea1_hash'].map(lambda x: len(str(x)))\ntest['fea_hash_len'] = test['fea_hash'].map(lambda x: len(str(x)))\ntest['fea1_hash_len'] = test['fea1_hash'].map(lambda x: len(str(x)))\n\n# 如果fea_hash很长或者很小，都归为0，否则为自己的本身\ntrain['fea_hash'] = train['fea_hash'].map(lambda x: 0 if len(str(x))>10 or len(str(x))&lt;=8 else int(x))\ntrain['fea1_hash'] = train['fea1_hash'].map(lambda x: 0 if len(str(x))&lt;=8 else int(x))\ntest['fea_hash'] = test['fea_hash'].map(lambda x: 0 if len(str(x))>10 or len(str(x))&lt;=8 else int(x))\ntest['fea1_hash'] = test['fea1_hash'].map(lambda x: 0 if len(str(x))&lt;=8 else int(x))\n\nall_df = pd.concat([train,test])\nall_df['fea_hash']= all_df['fea_hash'].astype('str')\nall_df['fea_hash'] = le.fit_transform(all_df['fea_hash'])\nall_df['fea1_hash']= all_df['fea1_hash'].astype('str')\nall_df['fea1_hash'] = le.fit_transform(all_df['fea1_hash'])\n\nfor fea in cate_features:\n    all_df[fea]= all_df[fea].astype('object')\n    all_df[fea] = le.fit_transform(all_df[fea])\n\nall_df = all_df.drop('dev_ppi',axis=1)\ntrain = all_df[all_df['label']!=-1]\ntest = all_df[all_df['label']==-1]\n\n模型训练feature = train.columns.tolist()\nfeature.remove('sid')\nfeature.remove('label')\ntrain[feature].info()\n'''\n&lt;class 'pandas.core.frame.DataFrame'>\nInt64Index: 500000 entries, 0 to 499999\nData columns (total 33 columns):\n #   Column                  Non-Null Count   Dtype  \n---  ------                  --------------   -----  \n 0   android_id              500000 non-null  int64  \n 1   apptype                 500000 non-null  int64  \n 2   carrier                 500000 non-null  int64  \n 3   dev_height              500000 non-null  int64  \n 4   dev_width               500000 non-null  int64  \n 5   lan                     500000 non-null  int64  \n 6   media_id                500000 non-null  int64  \n 7   ntt                     500000 non-null  int64  \n 8   osv                     500000 non-null  int64  \n 9   package                 500000 non-null  int64  \n 10  version                 500000 non-null  int64  \n 11  fea_hash                500000 non-null  int64  \n 12  location                500000 non-null  int64  \n 13  fea1_hash               500000 non-null  int64  \n 14  cus_type                500000 non-null  int64  \n 15  fea1_hash_len           500000 non-null  int64  \n 16  fea_hash_len            500000 non-null  int64  \n 17  dev_rate                500000 non-null  float64\n 18  dev_area                500000 non-null  float64\n 19  dev_area_ppi            500000 non-null  float64\n 20  dev_height_value_count  500000 non-null  int64  \n 21  dev_width_value_count   500000 non-null  int64  \n 22  media_id_value_count    500000 non-null  int64  \n 23  package_value_count     500000 non-null  int64  \n 24  apptype_value_count     500000 non-null  int64  \n 25  android_id_value_count  500000 non-null  int64  \n 26  fea1_hash_value_count   500000 non-null  int64  \n 27  fea_hash_value_count    500000 non-null  int64  \n 28  day                     500000 non-null  int64  \n 29  weekday                 500000 non-null  int64  \n 30  hour                    500000 non-null  int64  \n 31  minute                  500000 non-null  int64  \n 32  timestamp_diff          500000 non-null  int64  \ndtypes: float64(3), int64(30)\nmemory usage: 129.7 MB\n'''\nX = train[feature]\ny = train['label']\nX_test = test[feature]\n\n# LGB XGB 5折交叉验证\nfrom lightgbm import LGBMClassifier\ndef lgb_xgb_train(KF,X,y,X_test):\n    seed = 2022\n    lgb = LGBMClassifier(num_leaves=1024,\n                           max_depth=12,\n                           learning_rate=0.005,\n                           n_estimators=5000,\n                           subsample=0.8,\n                           feature_fraction=0.8,\n                           reg_alpha=0.5,\n                           reg_lambda=0.5,\n                           random_state=seed,\n                           metric='auc',\n                           boosting_type='gbdt',\n                           subsample_freq=1,\n                           bagging_fraction=0.8,verbose=-1)\n\n    xgb=XGBClassifier(\n            max_depth=13, learning_rate=0.005, n_estimators=2400, \n            objective='binary:logistic',\n            subsample=0.95, colsample_bytree=0.4,n_jobs=-1,random_state=seed, \n            min_child_samples=3, eval_metric='auc', reg_lambda=0.5,verbosity=0)\n\n    xgb_prob = np.zeros(X_test.shape[0])\n    lgb_prob = np.zeros(X_test.shape[0])\n    skf = StratifiedKFold(n_splits=KF, shuffle=True, random_state=2022)\n\n    for k,(train_index, test_index) in enumerate(skf.split(X, y)):\n        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n        print(f'第&#123;k+1&#125;次训练')\n\n        # 训练\n        xgb = xgb.fit(X_train,y_train,\n                      eval_metric='auc',eval_set=[(X_val,y_val)],\n                      verbose=False)\n        lgb = lgb.fit(X_train,y_train,\n                      eval_set=[(X_val, y_val)],eval_metric='auc',\n                              verbose = False)\n\n        # 正式预测\n        xgb_prob += xgb.predict_proba(X_test)[:,1]/KF\n        print(f'Xgboost:第&#123;k+1&#125;次训练的AUC&#123;roc_auc_score(y_val,xgb.predict_proba(X_val)[:,1])&#125;,第&#123;k+1&#125;次训练的Accuracy&#123;accuracy_score(y_val,xgb.predict(X_val))&#125;')\n        lgb_prob += lgb.predict_proba(X_test)[:,1]/KF\n        print(f'LightGBM:第&#123;k+1&#125;次训练的AUC&#123;roc_auc_score(y_val,lgb.predict_proba(X_val)[:,1])&#125;,第&#123;k+1&#125;次训练的Accuracy&#123;accuracy_score(y_val,lgb.predict(X_val))&#125;')\n\n    return xgb_prob,lgb_prob\n\n\nxgb_prob,lgb_prob = lgb_xgb_train(5,X,y,X_test)\nsubmit = test[['sid']]\nsubmit['xgb_prob'] = xgb_prob\nsubmit['lgb_prob'] = lgb_prob\nsubmit['融合'] = xgb_prob + lgb_prob\nsubmit['label'] = submit['融合'].apply(lambda x:1 if x>=1 else 0)\n\nTrick_id = [1260961, 1393628, 1356887, 1911347,1298847, 1629795]\nsubmit[submit['sid'].apply(lambda x:x in Trick_id)]['xgb_prob'] = 1\nsubmit[submit['sid'].apply(lambda x:x in Trick_id)]['lgb_prob'] = 1","slug":"数据竞赛实战（一）点击反欺诈预测，2022年3月第二名","date":"2022-06-18T02:40:00.000Z","categories_index":"数据挖掘","tags_index":"数据竞赛,xgboost,lightGBM","author_index":"风离"},{"id":"1254a8f3f75ef701c93024f8482cdb21","title":"NLP初步（三）预训练词向量加载代码实战","content":"大家好，我是丹星，一个摸鱼转行的炼丹师。今天的内容干货满满，全是实战内容哟。\n预训练词向量之前我们介绍了word2vec词向量的训练方法，其实还有更多的词向量训练方法，常见的比如glove和fasttext，感兴趣的朋友可以去另外了解，这里就不对原理进行太多的讲解。如果想要获取现成的词向量文件，可以去对应的官网上下载，当然在kaggle上有人整理好了现成的词向量放在了Dataset中，方便在kaggle跑代码时快速调用，以下列出几个本人经常用的Dataset：\n\nGlove ：https://www.kaggle.com/datasets/yesornope/glove6b\nFasttext：https://fasttext.cc/docs/en/english-vectors.html\n英文词向量汇总：https://www.kaggle.com/datasets/iezepov/gensim-embeddings-dataset?select=glove.twitter.27B.200d.gensim\n中文：https://www.kaggle.com/datasets?search=chinese+word+embedding\n\n代码实战既然有了预训练的词向量，我们要如何匹配自己的数据集，然后加载到模型的Embedding层中去呢？本文仍然以新冠推文情感分析的数据集为例，使用glove预训练词向量进行文本情感分类。\n代码仍然在kaggle公开：\nhttps://www.kaggle.com/code/leekemperor/pretrain-word-embedding-in-text-classification?scriptVersionId=98573268\n首先，导入相关的库\nimport os\nimport torch\nimport re\nimport copy\nimport torch.nn as nn\nfrom torch.nn import Embedding\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom torch import nn,optim\nfrom tqdm.notebook import tqdm\nfrom torch.optim import Adam\n加载数据和词向量文件\nroot_path = Path(\"../input/covid-19-nlp-text-classification\")  \ntrain_df = pd.read_csv(root_path / \"Corona_NLP_train.csv\",encoding='latin')  \ntest_df = pd.read_csv(root_path / \"Corona_NLP_test.csv\",encoding='latin')\n\nembedding_dim = 300  # 设定需要的embedding长度\n\n# word_dict是一个单词于id的映射字典，\n# &lt;pading>的意思是补0，&lt;unk>代表不认识的单词，其实就是glove词向量中没有的单词都会被认为是&lt;unk>\nword_dict = &#123;'&lt;pading>':0,\"&lt;unk>\": 1&#125;\n\n\n# 加载对应长度的glove预训练词向量，维度越大的词向量加载越慢，300维的词向量文件有1G\nglove_path = Path(\"/kaggle/input/glove6b\")  \nglove_df = pd.read_csv(glove_path / f\"glove.6B.&#123;embedding_dim&#125;d.txt\", sep=\" \", quoting=3, header=None, index_col=0)\n# 生成对应的字典形式，key为单词，value为词向量\nglove_dict = &#123;key: val.values for key, val in glove_df.T.items()&#125;\n\n\n\n对文本进行分词处理，并构建文本词表，输入模型的句子必须是一样长度，所以要设定一个长度，超出长度的部分截断，不足的部分补0（padding）。\ndef word_tokenize(text: str):\n    \"\"\" \n    这是一个切分单词的函数，这个函数除了简单了分词之外，\n    还会将word_dict补充完整，生成完整的词表映射\n    \"\"\"\n    word_index = []\n    pat = re.compile(r\"[\\w]+|[.,!?;|]\") \n    tokens = pat.findall(text.lower())  \n    for token in tokens:\n        if token not in word_dict:\n            word_dict[token] = len(word_dict) if token in glove_dict else word_dict[\"&lt;unk>\"]\n        word_index.append(word_dict[token])\n    return word_index\n\n# 训练集和测试集分词\ntrain_text = train_df[\"OriginalTweet\"].apply(lambda s: word_tokenize(str(s)))\ntest_text = test_df[\"OriginalTweet\"].apply(lambda s: word_tokenize(str(s)))\n\nlen(word_dict)  # 词表大小85091\n\ntrain_text.apply(lambda x:len(x)).describe()  # 设定句子最大长度为240\n'''\ncount    41157.000000\nmean        36.377093\nstd         13.705114\nmin          1.000000\n25%         26.000000\n50%         37.000000\n75%         47.000000\nmax        232.000000\nName: OriginalTweet, dtype: float64\n'''\n\nMAX_LENGTH = 240  # 最大句子长度\ntrain_text = train_text.apply(lambda x:(x+[0]*MAX_LENGTH)[:MAX_LENGTH])\ntest_text = test_text.apply(lambda x:(x+[0]*MAX_LENGTH)[:MAX_LENGTH])\n\ns2i = &#123;'Positive':1,'Extremely Positive':0,'Neutral':2,'Extremely Negative':4,'Negative':3&#125;\ntrain_df['Sentiment']=train_df['Sentiment'].replace(s2i).astype(int)\ntest_df['Sentiment']=test_df['Sentiment'].replace(s2i).astype(int)\n\n# 预训练的词向量矩阵，这个后面会直接输入embedding层\nglove_embeddings = np.zeros((len(word_dict), embedding_dim))\nfor k, v in word_dict.items():\n    if v==0:\n        glove_embeddings[v] = np.zeros(embedding_dim)\n    glove_embeddings[v] = glove_dict[k] if k in glove_dict else glove_dict[\"&lt;unk>\"]\n数据集构建\nclass CommentDataset(Dataset):\n    def __init__(self,texts,labels):\n        self.texts=texts\n        self.labels=labels\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self,item):\n        \"\"\"\n        item 为数据索引，迭代取第item条数据\n        \"\"\"\n        text=torch.tensor(self.texts[item],dtype=torch.long)\n        label=torch.tensor(self.labels[item],dtype=torch.long)\n        return &#123;'text_id':text,'label':label&#125;\ndef create_data_loader(X,y,batch_size):\n    ds=CommentDataset(\n        texts = X.values,\n        labels=y.values\n    )\n    return DataLoader(\n        ds,\n        batch_size=batch_size\n    )\n\nBATCH_SIZE = 128\ntrain_data_loader = create_data_loader(train_text,train_df['Sentiment'], BATCH_SIZE)\nval_data_loader = create_data_loader(test_text,test_df['Sentiment'], BATCH_SIZE)\n模型构建，这里搭了两个模型，第一个是词向量后接全连接层，第二个是接的GRU，之后看下这两种做法的表现怎样，虽然毋庸置疑是GRU更好。\nclass Model(nn.Module): \n    def __init__(self,weight, embedding_dim=300):\n        super(Model, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(weight,freeze=False)  # 仍然训练词向量\n        self.Linear = nn.Linear(embedding_dim,5)\n    def forward(self, X):\n        X = self.embedding(X) # [b,240] ==> [b,240,300]\n        X = X.mean(dim=1)  # [b,240] ==> [b,300]\n        out = self.Linear(X) # [b,300] ==> [b,5]\n        return out\n\nclass GRUModel(nn.Module): \n    def __init__(self,weight, embedding_dim=300,hidden_size=512):\n        super(GRUModel, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(weight,freeze=False)  # 仍然训练词向量\n        self.rnn = nn.GRU(300,hidden_size,batch_first=True,num_layers=2,bidirectional=True,dropout=0.3)\n        self.linear = nn.Linear(hidden_size*2,hidden_size)\n        self.dropout = nn.Dropout(0.5)\n        self.out = nn.Linear(hidden_size,5)\n    def forward(self, X):\n        X = self.embedding(X) # [b,240] ==> [b,240,300]\n        X,_ = self.rnn(X)     #  [b,240,300] ==>  [b, 240, hidden_size*2]\n        X = self.dropout(self.linear(X)) # [b, 240, hidden_size*2] ==> [b, 240, hidden_size]\n        X = X.mean(dim=1)  # [b, 240, hidden_size] ==> [b,hidden_size]\n        out = self.out(X) # [b,hidden_size] ==> [b,5]\n        return out\n模型训练\nfrom collections import defaultdict\nhistory = defaultdict(list)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 设置CUDA\nN_EPOCHS = 10     # 设置模型训练次数\nlearning_rate = 3e-4  # 初始学习率\n\nmodel= GRUModel(torch.from_numpy(glove_embeddings).float())\nmodel = model.to(device)\n# 优化器\noptimizer = Adam(model.parameters(),learning_rate)\n\n# 损失\ncriterion = nn.CrossEntropyLoss().to(device)\nfor epoch in tqdm(range(N_EPOCHS)):\n    #启用模型的训练模式\n    model.train()\n    # 定义损失\n    epoch_loss = 0\n    epoch_acc = 0\n    val_number = 0\n    for batch in train_data_loader:\n        text_id = batch['text_id'].to(device)\n        label = batch['label'].to(device)\n        predictions = model(text_id)\n        loss = criterion(predictions, label)\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        preds = predictions.max(1)[1]\n        epoch_acc += ((preds==label).sum().item())\n        val_number += label.size(0)\n    history['train_loss'].append(epoch_loss / len(train_data_loader))\n    history['train_accuracy'].append(epoch_acc / val_number)\n    print(f'第&#123;epoch+1&#125;轮，训练Loss：',epoch_loss / len(train_data_loader),'，训练准确率：',epoch_acc / val_number)\n\n    model.eval()\n    # 初始化损失\n    epoch_loss = 0\n    epoch_acc = 0\n    val_number = 0\n\n    #不计算梯度\n    with torch.no_grad():\n        for batch in val_data_loader:\n            text_id = batch['text_id'].to(device)\n            label = batch['label'].to(device)\n            predictions = model(text_id)\n            loss = criterion(predictions, label)\n            epoch_loss += loss.item()\n            preds = predictions.max(1)[1]\n            epoch_acc += ((preds==label).sum().item())\n            val_number += label.size(0)\n    history['test_loss'].append(epoch_loss / len(val_data_loader))\n    history['test_accuracy'].append(epoch_acc / val_number)\n    print(f'第&#123;epoch+1&#125;轮，测试Loss：',epoch_loss / len(val_data_loader),'，测试准确率：',epoch_acc / val_number)\n    print('-'*20)\n\n\n\n可以看到GRU还是具有明显优势的，收敛轮数少，并且分数高。当然RNN的显著缺点之一就是时间开销比较大。\n","slug":"NLP初步（三）","date":"2022-06-17T02:35:00.000Z","categories_index":"NLP","tags_index":"NLP,数据挖掘,深度学习","author_index":"风离"},{"id":"fee5334a6e9b786f2386b28178cc2265","title":"NLP初步（二）从TF-IDF到Word2Vec","content":"大家好，这里是丹星，一个摸鱼转行的炼丹师。今天我们来聊一聊词袋模型的简单进化版——TFIDF，以及之后主流的文本表示方式词向量的开山之作——Wrod2Vec。\nTF-IDF昨天我们谈到词袋模型其实就是将每个句子的词频转化为了一个高维稀疏的向量，TF—IDF正是基于词袋的结果进行了改进，或者直白的说，就是词袋的结果乘一个权重。\n词袋的表示\ntf-idf的表示\nn表示文档中所有词出现的次数总和，tf可以看作是一个归一化的过程，值域为[0,1]。D表示语料中所有的文档总数，d表示语料中出现某个词的文档数量，分母加1是为了保证不为0。\n举个栗子，如果有一个词在所有的文档中都有，比如中文的介词“的”，那么它的idf值就会趋近于0（因为d&#x3D;D），相当于权重趋近于0。如果有一些词语只在少数文档中出现，那么我们可以认为这些词出现的文档具有某种相同的特质，比如一些法律类的专有名词可能会在律法新闻中出现，而在其他领域的文本中出现次数较少，这时d相对于D来说比较小，idf会有比较大的值，这些词也有了更高的权重。\n代码实践仍然是用冠状病毒推文情感分析的数据集，地址：https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv\n分析思路和昨天的Bow代码一致，代码基本上没有变化。\n这两个模型的代码我都写在一个Notebook里面，已经在kaggle公开：https://www.kaggle.com/code/leekemperor/bow-tfidf\n有兴趣的小伙伴可以直接运行。\nWord2vecWord2vec在2013年由Google 公司开源提出。之前在说词袋模型的时候，我们说到了它并不能很好的表示句子上下文的信息，本质上仍然是一个频率的特征。我们希望得到一个数学性质更好的的文本表示方法，如果能够使得相近意思的词语，在数学上有着更强的关联性，或者说具有较高的相似度，这才是我们希望的文本表示方法。比如我们希望西瓜和橘子有着相近的表示，而猪肉和羊肉有着更相近的表示，从向量角度出发，这种相近的表示可以体现为两个向量相似度更大。\n我们先说word2vec的结果是什么再来谈过程，结果就是将一个词语转化乘一个向量，这个向量的维数一般是自己定义的，常见的有100，200，300，512，1024等。这些维度具体代表着什么呢？很抱歉，我们并不知道，但是我们可以知道的是在训练足够充分的前提下，越高维度的向量，具有更强的语义信息，能够更精确的表示文本。这些维度可以看成是隐藏的语义特征，我们无法知道其具体含义，但是神经网络自动帮我们训练了出来。\nskip-gram和CBOW\n如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』\n而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』\n\n我们似乎又回到了最开始的问题，既然要用一个词预测它周围的词，仍然要对文本进行数值表示，那么如何表示呢？\n答案是one-hot编码。\n什么是one-hot编码，比如在新冠推文的数据集中，我们总共有1W个单词，那么这个数据集中的每一个单词，我都可以把他表示为[0,0,0,0,...,1,0,0,0]这样一个形状为（1，10000）的稀疏向量。这个向量由一个1和9999个0组成，1的位置其实就是单词的序号，比如“hi”是0号元素，那么对应向量的1就标在0号位，其余位置都是1。\n\n\n以Skip-gram为例，对应上图Xk就是一个一万维度的one-hot向量（V=10000），Hidden Layer所含的隐藏层维度是自定义的词向量维度，比如为100（N=100）。Output的部分你可能会觉得很奇怪，这是个什么玩意？？？因为skip-gram我们是预测上下文，所以你要输入一个词，去预测多个词，就有了Output的这种并联的结构。但是这部分并不重要，因为重点其实在前两个部分。\n\n因为Xk中只有一个位置为1，所以全连接层的权重只有为1的那个位置是有效的，而这个位置会和隐藏层的100个神经元一对一的连接，产生了权重系数，得到了一个100维度权重向量。是不是发现了什么，没错，这个100维度的权重向量就是我们需要的词向量，经过不断的训练，这个权重会不断的优化，更加精确的表达每一个词的语义特征。\n\nCBOW模型其实类似，这中间其实还涉及到了一些Trick，比如负采样（因为上下文的抽样都是正样本，所以需要进行随机抽样产生一些负样本，保证模型更好的训练，说人话就是随便找一些不是上下文的词语放进模型中）和hierarchical softmax（把 N 分类问题变成 log(N)次二分类问题），这里就不过多介绍。\n\n代码实践之后的实战中，我们其实不会自己去训练词向量，所以我放了一个简单的Skip-gram Demo在kaggle上面，这个是训练数字和字母的词向量，最后将词向量降维可以得到下面那个图。这个代码也可以学习一些简单的pytorch操作，比如创建数据集和自定义Model\n地址：https://www.kaggle.com/code/leekemperor/word2vec-skip-gram\n模型示例（偷懒用Embedding）：\nclass SkipGram(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.embed = torch.nn.Embedding(30, 2)\n        self.embed.weight.data.normal_(0,0.1)\n\n        self.fc = torch.nn.Linear(2, 30)\n\n    def forward(self, x):\n\n        #[b] -> [b,2]\n        x = self.embed(x)\n\n        #[b,2] -> [b,30]\n        x = self.fc(x)\n\n        return x\n\n数字和字母词向量可视化：\n\n下一篇，我们将聊一下一些常见的预训练词向量，并且会说明如何将预训练的词向量对应自己的数据集并加载到自己模型的Embedding层中。\n\n\n\n\n\n\n\n\n\n欢迎关注这个摸鱼更新的公众号\n \n","slug":"NLP初步（二）","date":"2022-06-16T04:00:00.000Z","categories_index":"NLP","tags_index":"NLP,数据挖掘,深度学习","author_index":"风离"},{"id":"3a2a27adc97b520f5d4502f70de41100","title":"NLP初步（一）从词袋模型说开去","content":"大家好，这里是丹星，一个摸鱼转行的炼丹师。今天是僵尸公众号复活的第一天，从今天开始，我将不定期更新机器学习和深度学习相关方面知识。\n我们来简单介绍一下自然语言处理( Natural Language Processing, NLP)。NLP是人类语言、计算机科学和人工智能的一个子领域，是目前深度学习的一个分支方向。说得比较通俗一点，就是使得模型能够认识人类的语言，这里包括文本和语音，但是我在今后比较长的时间里不会涉及到语音的相关内容，如果感兴趣可以自行搜索李宏毅老师的公开课进行学习。\n进入正题，众所周知机器不能直接识别文本内容，包括众多编程语言中的字符串类型其实仍然是转换成了二进制来存储。因此，我们需要文字转化为机器能看懂的数字，然后再去做其他事，本文介绍早期的文本转化方法——**词袋模型（Bow)**。\nBow模型举个栗子，词袋模型的表达非常简洁，如下面str为文本，一共有两句话，word2index中写出了该段文本中所有的词，一共四个。那么我们默认从0到3号分别对应I到dog，则词袋模型表现为了bow的形式。\nstr = ['I love you','you love dog']   # 文本\n\nword2index = &#123;'I':0,'you':1,'love':2,'dog':3&#125; # 词的映射\n\n# 词袋模型\nbow = [\n    [1,1,1,0],\n    [0,1,1,1]\n]\n\n这就是最简单的词袋模型的表达，但需要注意的是，这里所有词只出现了一次。如果某句话dog出现了2次，那么对应位置（3号位）就会是2。说白了，词袋模型统计了所有句子中出现的词语，然后给某个词一个位置，每句话的对应位置代表了该词在句子中出现的个数，《精通特征工程》所画的示意图如下\n\n优点\n比较简单，便于理解\n\n缺点\n词袋的维数（dim）和文本所含的词数是正相关的，当文本过大，词数较多时，引发了维数灾难。\n忽略了文本中单词的顺序和结构信息，比如“我爱你”和“你爱我”在词袋模型的表述中是一样的。\n\n代码实践（Show Code)\n选用数据集为：冠状病毒推文 NLP - 文本分类，地址：https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv。\n各位可以自行下载。\nimport numpy as np \nimport pandas as pd \ndf=pd.read_csv('Corona_NLP_train.csv',encoding='latin')\n\n# 五分类的情感分析\ns2i = &#123;'Positive':1,'Extremely Positive':2,'Neutral':3,'Extremely Negative':5,'Negative':4&#125;\ndf['Sentiment']=df['Sentiment'].replace(s2i) # 情感映射成数字\ndf['Sentiment']=df['Sentiment'].astype(int)\n\n# sklearn的词袋模型函数，可以直接调用\nfrom sklearn.feature_extraction.text import CountVectorizer  \nfrom sklearn.linear_model import LogisticRegression  # 逻辑回归\nfrom sklearn.model_selection import train_test_split # 切分数据\nfrom sklearn import metrics       # 评估指标\n\nx=df['OriginalTweet']  # 这个是推文的文本\ny=df['Sentiment']\n# 切分训练集和测试集\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0)\nvect=CountVectorizer(min_df=5).fit(x_train)  # min_df=5：出现次数低于5次的词语忽略\nnames=np.array(vect.get_feature_names())   # names是词袋模型所有的词语\nx_train_trans=vect.transform(x_train)     # 将x_train转化为词袋模型表示\n\nlen(vect.get_feature_names())  # 9363，有9363个词，所以对应了9363个特征维度\nx_train_trans.shape            # (30867, 9363)\n\n# 逻辑回归训练\nmodel = LogisticRegression(C=1)\nmodel.fit(x_train_trans, y_train)\npredict=model.predict(vect.transform(x_test))  # 在测试集上预测\nprint('Auccuracy &#123;:.2%&#125;'.format(metrics.accuracy_score(y_test,predict))) # Auccuracy 61.53%\nprint('F1_score &#123;:.2%&#125;'.format(metrics.f1_score(y_test,predict,average='macro'))) # F1_score 62.08%\nprint(metrics.classification_report(y_test,predict,target_names=list(s2i.keys())))\n\n分类报告结果图：\n\n\n\n\n\n\n\n\n\n\n欢迎关注这个摸鱼更新的公众号\n","slug":"NLP初步（一）词袋","date":"2022-06-15T05:01:00.000Z","categories_index":"NLP","tags_index":"NLP,数据挖掘","author_index":"风离"}]