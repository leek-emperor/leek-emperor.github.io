{"title":"NLP初步（二）从TF-IDF到Word2Vec","uid":"fee5334a6e9b786f2386b28178cc2265","slug":"NLP初步（二）","date":"2022-06-16T04:00:00.000Z","updated":"2022-10-05T13:37:51.067Z","comments":true,"path":"api/articles/NLP初步（二）.json","keywords":null,"cover":"https://res.cloudinary.com/dg7crzfct/image/upload/v1664976916/%E6%96%87%E7%AB%A0%E7%B4%A0%E6%9D%90%E5%BA%93/NLP%E7%9B%B8%E5%85%B3/tonmoy-iftekhar--Ryi8beHwgg-unsplash_blf8bk.jpg","content":"<p>大家好，这里是丹星，一个摸鱼转行的炼丹师。今天我们来聊一聊词袋模型的简单进化版——TFIDF，以及之后主流的文本表示方式词向量的开山之作——Wrod2Vec。</p>\n<h1 id=\"TF-IDF\"><a href=\"#TF-IDF\" class=\"headerlink\" title=\"TF-IDF\"></a>TF-IDF</h1><p>昨天我们谈到词袋模型其实就是将每个句子的词频转化为了一个高维稀疏的向量，TF—IDF正是基于词袋的结果进行了改进，或者直白的说，就是词袋的结果乘一个权重。</p>\n<h2 id=\"词袋的表示\"><a href=\"#词袋的表示\" class=\"headerlink\" title=\"词袋的表示\"></a>词袋的表示</h2><p><img src=\"https://res.cloudinary.com/dg7crzfct/image/upload/v1664976223/%E6%96%87%E7%AB%A0%E7%B4%A0%E6%9D%90%E5%BA%93/NLP%E7%9B%B8%E5%85%B3/640_1_hc1h9a.png\"></p>\n<h2 id=\"tf-idf的表示\"><a href=\"#tf-idf的表示\" class=\"headerlink\" title=\"tf-idf的表示\"></a>tf-idf的表示</h2><p><img src=\"https://res.cloudinary.com/dg7crzfct/image/upload/v1664976362/%E6%96%87%E7%AB%A0%E7%B4%A0%E6%9D%90%E5%BA%93/NLP%E7%9B%B8%E5%85%B3/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20221005212540_kcbwkz.png\"></p>\n<p>n表示文档中所有词出现的次数总和，tf可以看作是一个归一化的过程，值域为[0,1]。D表示语料中所有的文档总数，d表示语料中出现某个词的文档数量，分母加1是为了保证不为0。</p>\n<p>举个栗子，如果有一个词在所有的文档中都有，比如中文的介词“的”，那么它的idf值就会趋近于0（因为d&#x3D;D），相当于权重趋近于0。如果有一些词语只在少数文档中出现，那么我们可以认为这些词出现的文档具有某种相同的特质，比如一些<strong>法律类的专有名词可能会在律法新闻中出现，而在其他领域的文本中出现次数较少</strong>，这时d相对于D来说比较小，idf会有比较大的值，这些词也有了更高的权重。</p>\n<h2 id=\"代码实践\"><a href=\"#代码实践\" class=\"headerlink\" title=\"代码实践\"></a>代码实践</h2><p>仍然是用冠状病毒推文情感分析的数据集，地址：<a href=\"https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv\">https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv</a></p>\n<p>分析思路和昨天的Bow代码一致，代码基本上没有变化。</p>\n<p>这两个模型的代码我都写在一个Notebook里面，已经在kaggle公开：<a href=\"https://www.kaggle.com/code/leekemperor/bow-tfidf\">https://www.kaggle.com/code/leekemperor/bow-tfidf</a></p>\n<p>有兴趣的小伙伴可以直接运行。</p>\n<h1 id=\"Word2vec\"><a href=\"#Word2vec\" class=\"headerlink\" title=\"Word2vec\"></a>Word2vec</h1><p>Word2vec在2013年由Google 公司开源提出。之前在说词袋模型的时候，我们说到了它并不能很好的表示句子上下文的信息，本质上仍然是一个频率的特征。我们希望得到一个数学性质更好的的文本表示方法，如果能够使得相近意思的词语，在数学上有着更强的关联性，或者说具有较高的相似度，这才是我们希望的文本表示方法。比如我们希望<strong>西瓜和橘子有着相近的表示，而猪肉和羊肉有着更相近的表示</strong>，从向量角度出发，这种相近的表示可以体现为两个向量相似度更大。</p>\n<p>我们先说word2vec的结果是什么再来谈过程，结果就是将一个词语转化乘一个向量，这个向量的维数一般是自己定义的，常见的有100，200，300，512，1024等。这些维度具体代表着什么呢？很抱歉，我们并不知道，但是我们可以知道的是<strong>在训练足够充分的前提下，越高维度的向量，具有更强的语义信息，能够更精确的表示文本。</strong>这些维度可以看成是隐藏的语义特征，我们无法知道其具体含义，但是神经网络自动帮我们训练了出来。</p>\n<h2 id=\"skip-gram和CBOW\"><a href=\"#skip-gram和CBOW\" class=\"headerlink\" title=\"skip-gram和CBOW\"></a>skip-gram和CBOW</h2><ul>\n<li>如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』</li>\n<li>而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』</li>\n</ul>\n<p style=\"text-indent:2em\">我们似乎又回到了最开始的问题，既然要用一个词预测它周围的词，仍然要对文本进行数值表示，那么如何表示呢？</p>\n<p style=\"text-indent:2em\">答案是one-hot编码。</p>\n<p style=\"text-indent:2em\">什么是one-hot编码，比如在新冠推文的数据集中，我们总共有1W个单词，那么这个数据集中的每一个单词，我都可以把他表示为[0,0,0,0,...,1,0,0,0]这样一个形状为（1，10000）的稀疏向量。这个向量由一个1和9999个0组成，1的位置其实就是单词的序号，比如“hi”是0号元素，那么对应向量的1就标在0号位，其余位置都是1。</p>\n\n<p><img src=\"https://res.cloudinary.com/dg7crzfct/image/upload/v1664976223/%E6%96%87%E7%AB%A0%E7%B4%A0%E6%9D%90%E5%BA%93/NLP%E7%9B%B8%E5%85%B3/640_3_jcdu6i.png\"></p>\n<p style=\"text-indent:2em\">以Skip-gram为例，对应上图Xk就是一个一万维度的one-hot向量（V=10000），Hidden Layer所含的隐藏层维度是自定义的词向量维度，比如为100（N=100）。Output的部分你可能会觉得很奇怪，这是个什么玩意？？？因为skip-gram我们是预测上下文，所以你要输入一个词，去预测多个词，就有了Output的这种并联的结构。但是这部分并不重要，因为重点其实在前两个部分。</p>\n\n<p style=\"text-indent:2em\">因为Xk中只有一个位置为1，所以全连接层的权重只有为1的那个位置是有效的，而这个位置会和隐藏层的100个神经元一对一的连接，产生了权重系数，得到了一个100维度权重向量。是不是发现了什么，没错，这个100维度的权重向量就是我们需要的词向量，经过不断的训练，这个权重会不断的优化，更加精确的表达每一个词的语义特征。</p>\n\n<p style=\"text-indent:2em\">CBOW模型其实类似，这中间其实还涉及到了一些Trick，比如负采样（因为上下文的抽样都是正样本，所以需要进行随机抽样产生一些负样本，保证模型更好的训练，说人话就是随便找一些不是上下文的词语放进模型中）和hierarchical softmax（把 N 分类问题变成 log(N)次二分类问题），这里就不过多介绍。</p>\n\n<h2 id=\"代码实践-1\"><a href=\"#代码实践-1\" class=\"headerlink\" title=\"代码实践\"></a>代码实践</h2><p>之后的实战中，我们其实不会自己去训练词向量，所以我放了一个简单的Skip-gram Demo在kaggle上面，这个是训练数字和字母的词向量，最后将词向量降维可以得到下面那个图。这个代码也可以学习一些简单的pytorch操作，比如创建数据集和自定义Model</p>\n<p>地址：<a href=\"https://www.kaggle.com/code/leekemperor/word2vec-skip-gram\">https://www.kaggle.com/code/leekemperor/word2vec-skip-gram</a></p>\n<p>模型示例（偷懒用Embedding）：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">SkipGram</span><span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>embed <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>embed<span class=\"token punctuation\">.</span>weight<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>normal_<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>fc <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n        <span class=\"token comment\">#[b] -> [b,2]</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>embed<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\">#[b,2] -> [b,30]</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> x<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>数字和字母词向量可视化：</p>\n<p><img src=\"https://res.cloudinary.com/dg7crzfct/image/upload/v1664976223/%E6%96%87%E7%AB%A0%E7%B4%A0%E6%9D%90%E5%BA%93/NLP%E7%9B%B8%E5%85%B3/640_4_h419y2.png\"></p>\n<p>下一篇，我们将聊一下一些常见的预训练词向量，并且会说明如何将预训练的词向量对应自己的数据集并加载到自己模型的Embedding层中。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>欢迎关注这个摸鱼更新的公众号</p>\n<p> <img src=\"https://res.cloudinary.com/dg7crzfct/image/upload/v1664947192/%E6%96%87%E7%AB%A0%E7%B4%A0%E6%9D%90%E5%BA%93/qrcode_for_gh_6b9e2ea53ffc_258_lkxhbj.jpg\"></p></blockquote>\n","text":"大家好，这里是丹星，一个摸鱼转行的炼丹师。今天我们来聊一聊词袋模型的简单进化版——TFIDF，以及之后主流的文本表示方式词向量的开山之作——Wrod2Vec。 TF-IDF昨天我们谈到词袋模型其实就是将每个句子的词频转化为了一个高维稀疏的向量，TF—IDF正是基于词袋的结果进行了...","link":"","photos":[],"count_time":{"symbolsCount":"2.7k","symbolsTime":"2 mins."},"categories":[{"name":"NLP","slug":"NLP","count":3,"path":"api/categories/NLP.json"}],"tags":[{"name":"NLP","slug":"NLP","count":3,"path":"api/tags/NLP.json"},{"name":"数据挖掘","slug":"数据挖掘","count":3,"path":"api/tags/数据挖掘.json"},{"name":"深度学习","slug":"深度学习","count":2,"path":"api/tags/深度学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#TF-IDF\"><span class=\"toc-text\">TF-IDF</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%AF%8D%E8%A2%8B%E7%9A%84%E8%A1%A8%E7%A4%BA\"><span class=\"toc-text\">词袋的表示</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#tf-idf%E7%9A%84%E8%A1%A8%E7%A4%BA\"><span class=\"toc-text\">tf-idf的表示</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5\"><span class=\"toc-text\">代码实践</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Word2vec\"><span class=\"toc-text\">Word2vec</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#skip-gram%E5%92%8CCBOW\"><span class=\"toc-text\">skip-gram和CBOW</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-1\"><span class=\"toc-text\">代码实践</span></a></li></ol></li></ol>","author":{"name":"风离","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"略懂数据挖掘、NLP和推荐算法的前炼丹师，目前沉迷JS、React的前端硕狗。 <br /> @ <b>公众号：丹星X</b>","socials":{"github":"https://github.com/leek-emperor","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"https://www.zhihu.com/people/ma-xing-yu-71","csdn":"https://blog.csdn.net/weixin_44008395?type=blog","juejin":"https://juejin.cn/user/1042800253352664","customs":{}}},"mapped":true,"prev_post":{"title":"NLP初步（三）预训练词向量加载代码实战","uid":"1254a8f3f75ef701c93024f8482cdb21","slug":"NLP初步（三）","date":"2022-06-17T02:35:00.000Z","updated":"2022-10-05T13:48:21.938Z","comments":true,"path":"api/articles/NLP初步（三）.json","keywords":null,"cover":"https://res.cloudinary.com/dg7crzfct/image/upload/v1664977691/%E6%96%87%E7%AB%A0%E7%B4%A0%E6%9D%90%E5%BA%93/NLP%E7%9B%B8%E5%85%B3/steve-johnson-1pHVwou3mIM-unsplash_ofzkrb.jpg","text":"大家好，我是丹星，一个摸鱼转行的炼丹师。今天的内容干货满满，全是实战内容哟。 预训练词向量之前我们介绍了word2vec词向量的训练方法，其实还有更多的词向量训练方法，常见的比如glove和fasttext，感兴趣的朋友可以去另外了解，这里就不对原理进行太多的讲解。如果想要获取现...","link":"","photos":[],"count_time":{"symbolsCount":"7.7k","symbolsTime":"7 mins."},"categories":[{"name":"NLP","slug":"NLP","count":3,"path":"api/categories/NLP.json"}],"tags":[{"name":"NLP","slug":"NLP","count":3,"path":"api/tags/NLP.json"},{"name":"数据挖掘","slug":"数据挖掘","count":3,"path":"api/tags/数据挖掘.json"},{"name":"深度学习","slug":"深度学习","count":2,"path":"api/tags/深度学习.json"}],"author":{"name":"风离","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"略懂数据挖掘、NLP和推荐算法的前炼丹师，目前沉迷JS、React的前端硕狗。 <br /> @ <b>公众号：丹星X</b>","socials":{"github":"https://github.com/leek-emperor","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"https://www.zhihu.com/people/ma-xing-yu-71","csdn":"https://blog.csdn.net/weixin_44008395?type=blog","juejin":"https://juejin.cn/user/1042800253352664","customs":{}}}},"next_post":{}}